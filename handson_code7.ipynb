{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T18:10:15.125647Z",
     "start_time": "2020-08-02T18:10:15.124129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble Learning is where the aggretate of the predictions from a ensemble of predictors (such as classifiers or\n",
    "# regressors) is used to get a better prediction than the output of the best individual predictor. An example is where a\n",
    "# group of Decision Tree classifiers are trained each on a different random subset of the training set - predictions are\n",
    "# made by aggregating the predictions of all the individual trees, then prediciting the class that gets the most votes.\n",
    "# This ensemble is called a Random Forest which while simple is one of the most powerful ML algorithms available.\n",
    "\n",
    "# Ensemble methods are often used near the end of a project - once you have a few good predictors they can be combined into\n",
    "# a superior one (used in the winning solution to the Netflix Prize).\n",
    "# The most popular Ensemble methods include bagging, boosting, and stacking, as well as Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T18:18:21.604951Z",
     "start_time": "2020-08-02T18:18:21.603194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Voting Classifiers:\n",
    "# Suppose there are a few trained classifiers (say a Logistic Regression classifier, SVM, Random Forest, and KNN) each with\n",
    "# around 80% accuracy - a simple way to create a better classifier is to aggregate the predicitions of each and predict the\n",
    "# class that gets the most votes. This majority-vote classifier is called a hard voting classifier. This voting classifier\n",
    "# often achieves higher accuracy than the best classifier in the ensemble. Even if each classifier is a weak learner (they\n",
    "# only do slightly better than random guessing), the ensemble can be a strong learner provided there are a sufficient\n",
    "# number of weak learners and they are sufficiently diverse.\n",
    "\n",
    "# How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased\n",
    "# coin that has a 51% chance of coming up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will\n",
    "# generally get more or less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you will find that\n",
    "# the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more you toss the coin, the\n",
    "# higher the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This is due to the law of large\n",
    "# numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads (51%).\n",
    "# Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time\n",
    "# (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy!\n",
    "# However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly\n",
    "# not the case because they are trained on the same data. They are likely to make the same types of errors, so there\n",
    "# will be many majority votes for the wrong class, reducing the ensemble’s accuracy.\n",
    "\n",
    "# Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse\n",
    "# classifiers is to train them using very different algorithms. This increases the chance that they will make very different\n",
    "# types of errors, improving the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T18:54:07.235327Z",
     "start_time": "2020-08-02T18:54:07.043007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T18:54:10.253438Z",
     "start_time": "2020-08-02T18:54:10.055285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "# If all classifiers are able to estimate class probabilities (i.e., they all have a predict_proba() method), then you can\n",
    "# tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers.\n",
    "# This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly\n",
    "# confident votes. All you need to do is replace voting=\"hard\" with voting=\"soft\" and ensure that all classifiers can\n",
    "# estimate class probabilities. This is not the case for the SVC class by default, so you need to set its probability\n",
    "# hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down\n",
    "# training, and it will add a predict_proba() method).\n",
    "\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:04:13.794785Z",
     "start_time": "2020-08-02T19:04:13.792890Z"
    }
   },
   "outputs": [],
   "source": [
    "# As discussed, one way to get a diverse set of classifiers is to use very different training algorithms. Another approach\n",
    "# is to use the same training algorithm for every predictor and train them on different random subsets of the training set.\n",
    "# When this sampling is performed with replacement, the method is called bagging (short for bootstrap aggregating, where\n",
    "# bootstrapping in statistics is just resampling with replacement). When sampling is performed without replacement, it is\n",
    "# called pasting. Bagging and pasting both allow training instances to be sampled several times across multiple predictors,\n",
    "# but only bagging allows training instances to be sampled several times for the same predictor. Note that both the training\n",
    "# of the predictors and the predictions themselves can be performed in parallel, via different CPU cores or servers, so\n",
    "# bagging and pasting scale very well.\n",
    "\n",
    "# Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the\n",
    "# predictions of all predictors. The aggregation function is typically the statistical mode (i.e. the most frequent\n",
    "# prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual\n",
    "# predictor has a higher bias than if it were trained on the original training set (since training on a reduced dataset\n",
    "# means the hypothesis set is likely to be further away from the optimal hypothesis), but aggregation reduces both bias\n",
    "# and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single\n",
    "# predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:17:45.099623Z",
     "start_time": "2020-08-02T19:17:44.871397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.856\n",
      "BaggingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class (or BaggingRegressor for\n",
    "# regression). The following code trains an ensemble of 500 Decision Tree classifiers:5 each is trained on 100 training\n",
    "# instances randomly sampled from the training set with replacement (for pasting, bootstrap=False). The n_jobs parameter\n",
    "# specifies the number of CPU cores to use for training and predicitions.\n",
    "# N.B. max_samples can alternatively by between 0.0 and 1.0, in which case the max number of training instances to sample\n",
    "# is equal to the size of the training set * max_samples.\n",
    "# Also, BaggingCLassifier automatically performs soft voting instead of hard voting if the base classifier can estimate\n",
    "# probabilities (i.e. it has a predict_proba() method), which is the case with Decision Tree classifiers.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "for clf in (tree_clf, bag_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:11:54.381472Z",
     "start_time": "2020-08-02T19:11:54.348551Z"
    }
   },
   "outputs": [],
   "source": [
    "# The decision boundary of the bagging ensemble of 500 trees is much smoother and more reasonable than that of the single\n",
    "# Decision Tree and will likely generalize better: the ensemble has comparable bias but smaller variance (it makes around\n",
    "# the same number of errors on the training set, but the decision boundary is less irregular).\n",
    "# Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a\n",
    "# slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated,\n",
    "# so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is generally\n",
    "# preferred. However, if you have spare time and CPU power, you can use cross-validation to evaluate both bagging and\n",
    "# pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:50:24.761150Z",
     "start_time": "2020-08-02T19:50:24.545123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013333333333333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all.\n",
    "# By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the\n",
    "# entire training set. This means that, due to the replacement, only about 63% of the training instances are sampled on\n",
    "# average for each predictor (as m (the sample size) grows, the ratio approaches 1 - exp(-1) = 63.2%) The remaining 37% of\n",
    "# the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "# for all predictors. Not sure how oob evaluation works when max_samples, the number of training instances to draw, is less\n",
    "# than the size of the dataset. Say max_samples=100, then the model is trained on 100 instances picked from X with\n",
    "# replacement, but how does it pick the instances for the oob evaluation...\n",
    "\n",
    "\n",
    "# Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need\n",
    "# for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each\n",
    "# predictor. In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic oob\n",
    "# evaluation after training.\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:50:26.228774Z",
     "start_time": "2020-08-02T19:50:26.082490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the oob evaluation suggests that the accuracy on the test_set should be around 90.1%\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T19:55:20.633581Z",
     "start_time": "2020-08-02T19:55:20.630437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.31746032, 0.68253968],\n",
       "        [0.34117647, 0.65882353],\n",
       "        [1.        , 0.        ],\n",
       "        [0.        , 1.        ],\n",
       "        [0.        , 1.        ],\n",
       "        [0.08379888, 0.91620112],\n",
       "        [0.31693989, 0.68306011],\n",
       "        [0.02923977, 0.97076023],\n",
       "        [0.97687861, 0.02312139],\n",
       "        [0.97765363, 0.02234637]]),\n",
       " 375,\n",
       " 375,\n",
       " 125)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The oob_devision_function_ variable (since the base esitmator has a predict_proba() method) gives the class probabilities\n",
    "# for each training instance. The first training instance has 31.75% of being in the negative class and 68.25% positive.\n",
    "bag_clf.oob_decision_function_[:10], len(\n",
    "    bag_clf.oob_decision_function_), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T20:21:31.285664Z",
     "start_time": "2020-08-02T20:21:31.284302Z"
    }
   },
   "outputs": [],
   "source": [
    "# The BaggingClassifier also supports sampling of the features as well, controlled by the hyperparameters max_features\n",
    "# and bootstrap_features (work the same way as max_samples and bootstrap) - so each predictor will be trained on a random\n",
    "# subset of the input features. This is particularly useful when dealing with high-dimensional inputs (such as images).\n",
    "# Sampling the training instances and features is called the Random Patches method, while keeping all training instances\n",
    "# (setting bootstrap=False and max_samples=1.0) but sampling features is called the Random Subspaces method.\n",
    "# Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T20:33:23.657740Z",
     "start_time": "2020-08-02T20:33:22.803471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.912\n",
      "BaggingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# Random Forests are simply ensembles of Decision Trees, geneerally trained via the bagging method (or sometimes pasting),\n",
    "# typically with max_samples set to the size of the training set. Instead of using a BaggingClassifier and passing it a\n",
    "# DecisionTreeClassifier, there is a convenient and optimized RandomForestClassifier (and RandomForestRegressor) - note\n",
    "# that the BaggingClassifier is still useful if you want to bag something other than Decision Trees. With a few exceptions,\n",
    "# RandomForestCLassifier has all the hyperparameters of DecisionTreeClassifier (to control how the trees are grown) and of\n",
    "# BaggingClassifier (to control the ensemble itself).\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                 max_leaf_nodes=16, n_jobs=-1,\n",
    "                                 random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(\n",
    "        splitter='random', max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (rnd_clf, bag_clf):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T20:35:37.079445Z",
     "start_time": "2020-08-02T20:35:37.077585Z"
    }
   },
   "outputs": [],
   "source": [
    "# When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for\n",
    "# splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for\n",
    "# each feature rather than searching for the best possible thresholds (like regular Decision Trees do). A forest of such\n",
    "# extremely random trees is called an Extremely Randomized Trees ensemble (or Extra-Trees for short). Once again, this\n",
    "# technique trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random\n",
    "# Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming\n",
    "# tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class.\n",
    "# Its API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API\n",
    "# as the RandomForestRegressor class.\n",
    "# It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier.\n",
    "# Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using\n",
    "# grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T20:50:33.139924Z",
     "start_time": "2020-08-02T20:50:32.508758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.11249225099876375\n",
      "sepal width (cm) 0.02311928828251033\n",
      "petal length (cm) 0.4410304643639577\n",
      "petal width (cm) 0.4233579963547682\n"
     ]
    }
   ],
   "source": [
    "# Another quality of Random Forests is that the make it easy to measure the relative importance of each feature.\n",
    "# Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity\n",
    "# on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal\n",
    "# to the number of training samples that are associated with it (see Chapter 6). Scikit-Learn computes this score\n",
    "# automatically for each feature after training, then it scales the results so that the sum of all importances is equal\n",
    "# to 1. You can access the result using the feature_importances_ variable. For example, the following code trains a\n",
    "# RandomForestClassifier on the iris dataset and outputs each feature’s importance. It seems that the most important \n",
    "# features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison \n",
    "# (11% and 2%, respectively):\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X, y)\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n",
    "    \n",
    "# Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need\n",
    "# to perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T21:26:15.612522Z",
     "start_time": "2020-08-02T21:26:15.610746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Boosting (or hypothesis booding) refers to any Ensemble method that can combine several weak learners into a strong one\n",
    "# (high accuracy). The general idea is to train predictors sequentially, each trying to correct its predecessor. The most\n",
    "# popular boosting methods are AdaBoost (Adaptive Boosing) and Gradient Boosting.\n",
    "\n",
    "# AdaBoost forces the next predictor in the sequence to focus more on the training instances that the predecessor\n",
    "# underfitted - this results in new predictors focusing more and more on the hard cases. For example, the algorithm first\n",
    "# trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm\n",
    "# then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the\n",
    "# updated weights, and makes predictions on the training set, updates the weights, and so on. Note that SVMs are generally\n",
    "# not good base predictors for AdaBoost since they are slow and tend to be unstable with it.\n",
    "\n",
    "# The base predictor generally gets many instances wrong, so their weights get boosted. The second classifier therefore\n",
    "# does a better job on these instances, and so on. AdaBoost has an associated learning rate - if this is halved, the\n",
    "# misclassified instance weights are boosted half as much at every iteration). This sequential learning technique has some\n",
    "# similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost\n",
    "# function, AdaBoost adds predictors to the ensemble, gradually making it better. Once all the predictors are trained,\n",
    "# the ensemble makes predictions similar to bagging/pasting, except that predictors have different weights depending on\n",
    "# their overall accuracy on the weighted training set.\n",
    "\n",
    "# Note that this sequential learning technique cannot be parallelized (fully), since each predictor can only be trained\n",
    "# after the last one has been trained and evaluated. Hence, it does not scale as well as bagging/pasting.\n",
    "\n",
    "# See book for AdaBoost algorithm.\n",
    "\n",
    "# Scikit-Learn uses a multiclass version of AdaBoost called SAMME16 (Stagewise Additive Modeling using a Multiclass\n",
    "# Exponential loss function). When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can\n",
    "# estimate class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use a variant of SAMME\n",
    "# called SAMME.R (the R stands for “Real”), which relies on class probabilities rather than predictions and generally\n",
    "# performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T21:31:22.288782Z",
     "start_time": "2020-08-02T21:31:22.130088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=42,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code trains an AdaBoost classifier based on 200 Decision Stumps using Scikit-Learn’s AdaBoostClassifier\n",
    "# class (there is also an AdaBoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1 - in other words,\n",
    "# a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the\n",
    "# AdaBoostClassifier class:\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1, random_state=42), n_estimators=200,\n",
    "    algorithm='SAMME.R', learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# If the AdaBoost ensemble overfits the training set (fits the noise), try reducing the number of estimators or more\n",
    "# strongly regularize the base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T22:11:27.259201Z",
     "start_time": "2020-08-02T22:11:27.251671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting, like AdaBoost, sequentially adds predictors to an ensemble, each correcting its predecessor; however,\n",
    "# instead of tweaking the instance weights at every iteration like AdaBoost, this method tries to fit the new predictor \n",
    "# to the residual errors made by the previous predictor.\n",
    "# For an example, here is Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT) - simply a regression task\n",
    "# using Decision Trees as the base predictors. \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "#Now train a second DecisionTreeRegressor on the residual errors made by the first\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "#and a third...\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "#Now we have an ensemble of three trees - it can make predictions on a new instance by summing the predictions of each tree\n",
    "X_new = [[0.5]]\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T22:11:30.768842Z",
     "start_time": "2020-08-02T22:11:30.763919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRegressor class. Much like the\n",
    "# RandomForestRegressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth,\n",
    "# min_samples_leaf), as well as hyperparameters to control the ensemble training, like the number of trees (n_estimators).\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)\n",
    "gbrt.predict([[0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T22:13:29.116069Z",
     "start_time": "2020-08-02T22:13:29.075666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=56,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=42, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you\n",
    "# will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is\n",
    "# a regularization technique called shrinkage. Note that with a small learning rate, if n_estimators is too low then\n",
    "# the GBRT will underfit the training data, while if n_estimators is too high it will overfit).\n",
    "# To find the optimal number of trees, you can use early stopping. A simple way to implement this is use of the\n",
    "# staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of training\n",
    "# (with one tree, two trees, etc.).\n",
    "\n",
    "# The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to\n",
    "# find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "print(bst_n_estimators)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T22:13:57.124050Z",
     "start_time": "2020-08-02T22:13:57.087939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is also possible to implement early stopping by actually stopping training early (instead of training a large number\n",
    "# of trees first and then looking back to find the optimal number). You can do so by setting warm_start=True, which makes\n",
    "# Scikit- Learn keep existing trees when the fit() method is called, allowing incremental training. The following code\n",
    "# stops training when the validation error does not improve for five iterations in a row:\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break   # early stopping\n",
    "gbrt.n_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T22:54:46.120085Z",
     "start_time": "2020-08-02T22:54:46.097781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:54:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\n",
      "[23:54:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tvalidation_0-rmse:0.286719\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.258221\n",
      "[2]\tvalidation_0-rmse:0.232634\n",
      "[3]\tvalidation_0-rmse:0.210526\n",
      "[4]\tvalidation_0-rmse:0.190232\n",
      "[5]\tvalidation_0-rmse:0.172196\n",
      "[6]\tvalidation_0-rmse:0.156394\n",
      "[7]\tvalidation_0-rmse:0.142241\n",
      "[8]\tvalidation_0-rmse:0.129789\n",
      "[9]\tvalidation_0-rmse:0.118752\n",
      "[10]\tvalidation_0-rmse:0.108388\n",
      "[11]\tvalidation_0-rmse:0.100155\n",
      "[12]\tvalidation_0-rmse:0.09208\n",
      "[13]\tvalidation_0-rmse:0.084791\n",
      "[14]\tvalidation_0-rmse:0.078699\n",
      "[15]\tvalidation_0-rmse:0.073248\n",
      "[16]\tvalidation_0-rmse:0.069391\n",
      "[17]\tvalidation_0-rmse:0.066277\n",
      "[18]\tvalidation_0-rmse:0.063458\n",
      "[19]\tvalidation_0-rmse:0.060326\n",
      "[20]\tvalidation_0-rmse:0.0578\n",
      "[21]\tvalidation_0-rmse:0.055643\n",
      "[22]\tvalidation_0-rmse:0.053943\n",
      "[23]\tvalidation_0-rmse:0.053138\n",
      "[24]\tvalidation_0-rmse:0.052415\n",
      "[25]\tvalidation_0-rmse:0.051821\n",
      "[26]\tvalidation_0-rmse:0.051226\n",
      "[27]\tvalidation_0-rmse:0.051135\n",
      "[28]\tvalidation_0-rmse:0.05091\n",
      "[29]\tvalidation_0-rmse:0.050893\n",
      "[30]\tvalidation_0-rmse:0.050725\n",
      "[31]\tvalidation_0-rmse:0.050471\n",
      "[32]\tvalidation_0-rmse:0.050285\n",
      "[33]\tvalidation_0-rmse:0.050492\n",
      "[34]\tvalidation_0-rmse:0.050348\n",
      "Stopping. Best iteration:\n",
      "[32]\tvalidation_0-rmse:0.050285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of training\n",
    "# instances to be used for training each tree. For example, if subsample=0.25, then each tree is trained on 25% of the\n",
    "# training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a\n",
    "# lower variance. It also speeds up training considerably. This is called Stochastic Gradient Boosting.\n",
    "# It is also possible to use Gradient Boosting with other cost functions, controlled by the loss hyperparameter (default\n",
    "# is least-squares, see docs for more info).\n",
    "\n",
    "# XGBoost (Extreme Gradient Boosing) is a Python library with an optimized implementation of Gradient Boosting, aiming to\n",
    "# be extremely fast, scalable and portible - it is often used as a component in winning entries to ML competitions. It has\n",
    "# an API quite similar to Scikit-Learn's:\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "print()\n",
    "# XGBoost also offers features such as automatically taking care of early stopping:\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking (short for stacked generalization) involves training a model to perform the aggregation itself, instead of using\n",
    "# trivial functions (such as hard voting) to aggregate the predictions of all the predictors in te ensemble. This model\n",
    "# is called a blender, or meta learner, and would take in the regression predictions from the individual models as an input\n",
    "# before making the final prediction.\n",
    "# Blenders are generally trained using a hold-out set (or by using out-of-fold predictions). (1) The training set is split\n",
    "# into two subsets. The first is used to train the predictors in the first layer. (2) The first layer predictors are used\n",
    "# to make predictions on the second (held-out) set. This ensures the predictions are 'clean' since the predictors never\n",
    "# saw them during training. If there are N predictors in the first layer, then for each instance in the hold-out set\n",
    "# there are N predicted values (so it effectively becomes a new training set which is N-dimensional with the same target\n",
    "# values). (3) The blender is trained on this new training set, so it learns to predict the target value given the first\n",
    "# layer's predictions.\n",
    "\n",
    "# It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using\n",
    "# Random Forest Regression), to get a whole layer of blenders. The trick is to split the training set into three subsets:\n",
    "# the first one is used to train the first layer, the second one is used to create the training set used to train the\n",
    "# second layer (using predictions made by the predictors of the first layer), and the third one is used to create the\n",
    "# training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done,\n",
    "# we have the single final blender in its own third layer and we can make a prediction for a new instance by going through\n",
    "# each layer sequentially\n",
    "\n",
    "# Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own\n",
    "# implementation (see the exercises). Alternatively, you can use an open source implementation such as DESlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
