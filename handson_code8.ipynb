{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.658266Z",
     "start_time": "2020-08-03T22:46:38.656033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only do all\n",
    "# these features make training extremely slow, but they can also make it much harder to find a good solution, as we will\n",
    "# see. This problem is often referred to as the curse of dimensionality. Fortunately, in real-world problems, it is often\n",
    "# possible to reduce the number of features considerably, turning an intractable problem into a tractable one. For example\n",
    "# consider the MNIST images: the pixels on the image borders are almost always white, so you could completely drop these\n",
    "# pixels from the training set without losing much information. Figure 7-6 confirms that these pixels are utterly\n",
    "# unimportant for the classification task. Additionally, two neighboring pixels are often highly correlated: if you merge\n",
    "# them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information.\n",
    "\n",
    "# Reducing dimensionality does cause some information loss (just like compressing an image to JPEG can degrade its\n",
    "# quality), so even though it will speed up training, it may make your system perform slightly worse. It also makes your\n",
    "# pipelines a bit more complex and thus harder to maintain. So, if training is too slow, you should first try to train\n",
    "# your system with the original data before considering using dimensionality reduction. In some cases, reducing the\n",
    "# dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher\n",
    "# performance, but in general it won’t; it will just speed up training.\n",
    "\n",
    "# Dimensionality reduction is also used for data visualization - reducing to 2D or 3D means we can plot a condensed view of\n",
    "# high-dimensional training data and often gain important insights by visually detecting patterns like clusters. DataViz\n",
    "# is also useful in communicating your conclusions to non data scientitsts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.662191Z",
     "start_time": "2020-08-03T22:46:38.659484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Curse of Dimensionality\n",
    "# In low dimensional space, the probability that a random point is 'extreme' or close to the border of some enclosing shape\n",
    "# is small (for instance in a unit square). In high dimensions, like a 10000-D hypercube, the probability is extremely\n",
    "# high - most points in such a hypercube are very close to the border (i.e. if you consider enough dimensions, the chance\n",
    "# that any one of the coordinates is close to the border is very high).\n",
    "\n",
    "# A more troublesome difference is that if you pick two points randomly in a unit square, the distance between these two\n",
    "# points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be\n",
    "# roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional hypercube? The average distance,\n",
    "# believe it or not, will be about 408.25 (roughly 1, 000, 000/6)! This is counterintuitive: how can two points be so far\n",
    "# apart when they both lie within the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a\n",
    "# result, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away\n",
    "# from each other. This also means that a new instance will likely be far away from any training instance, making\n",
    "# predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In\n",
    "# short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "# In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a\n",
    "# sufficient density of training instances. Unfortunately, in practice, the number of training instances required to\n",
    "# reach a given density grows exponentially with the number of dimensions. With just 100 features (significantly fewer\n",
    "# than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for\n",
    "# training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all\n",
    "# dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.665742Z",
     "start_time": "2020-08-03T22:46:38.663465Z"
    }
   },
   "outputs": [],
   "source": [
    "# The main approaches to dimensionality reduction are projection and Manifold Learning.\n",
    "# In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are\n",
    "# almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances\n",
    "# lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. For instance, points in 3D\n",
    "# space may lie close to a plane (a 2D subspace of the high dimensional 3D space). Projecting the data perpendicularly onto\n",
    "# this subspace, we get a 2D dataset, reducing the dimensionality and producing two new features for the data relative\n",
    "# to this 2D subspace (the coordinates of the projections).\n",
    "\n",
    "# Projection is not always the best approach - in many cases the subspace may twist and turn, such as for the 3D Swiss Roll\n",
    "# toy dataset. Projecting this onto a plane (such as by dropping x3) squashes different layers of the roll together -\n",
    "# ideally we would want to 'unroll' the data onto a 2D subspace (very likely with new features). This Swiss Roll is a form\n",
    "# of 2D manigold - a 2D shape that can be bent and twisted into a higher-dimensional space. In general a d-dim manifold\n",
    "# is part of an n-dim space (where d < n) that locally resembles a d-dim hyperplane (so above, d=2, n=3: it resembles\n",
    "# a 2D plane but is rolled in the third dimension).\n",
    "\n",
    "# Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is\n",
    "# called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that\n",
    "# most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often\n",
    "# empirically observed.\n",
    "# Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They are made of\n",
    "# connected lines, the borders are white, and they are more or less centered. If you randomly generated images, only a\n",
    "# ridiculously tiny fraction of them would look like handwritten digits. In other words, the degrees of freedom available\n",
    "# to you if you try to create a digit image are dramatically lower than the degrees of freedom you would have if you were\n",
    "# allowed to generate any image you wanted. These constraints tend to squeeze the dataset into a lower-dimensional\n",
    "# manifold.\n",
    "# The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification\n",
    "# or regression) will be simpler if expressed in the lower-dimensional space of the manifold. For example, in the top row\n",
    "# of Figure 8-6 the Swiss roll is split into two classes: in the 3D space (on the left), the decision boundary would be\n",
    "# fairly complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a straight line.\n",
    "# However, this implicit assumption does not always hold. For example, in the bottom row of Figure 8-6, the decision\n",
    "# boundary is located at x1 = 5. This decision boundary looks very simple in the original 3D space (a vertical plane), but\n",
    "# it looks more complex in the unrolled manifold (a collection of four independent line segments).\n",
    "# In short, reducing the dimensionality of your training set before training a model will usually speed up training, but\n",
    "# it may not always lead to a better or simpler solution; it all depends on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.694515Z",
     "start_time": "2020-08-03T22:46:38.666975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51194429 -0.17295472  0.84142719]\n",
      " [-0.85454939 -0.20232689  0.47834001]\n",
      " [-0.08751218  0.96392453  0.25137844]]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis (PCA) is the most popular dimenionality-reduction algorithm - it identifies the hyperplane\n",
    "# that lies closest to the data, then projects the data onto it.\n",
    "\n",
    "# First, the right hyperplane needs to be chosen. Of the many possible choices of hyperplane, we want to select the one\n",
    "# that preserves the maximum amount of variance in the data, as it will most likely lose less information than other\n",
    "# projections. In the case of a 2D dataset, this is the axis along which the maximum amount of variance is preserved.\n",
    "# In a different sense, this will be the same axis that minimizes the mean squared distance between the original dataset\n",
    "# and its projection onto that axis - this is the simple idea behind PCA.\n",
    "# Secondly, after finding the above axis it finds a second one, orthogonal to the first, which accounts for the\n",
    "# largest amount of remaining variance. The algorithm then finds a third, orthogonal to both previous axes, and a fourth,\n",
    "# and continues until it has as many axes as dimensionality of the dataset.\n",
    "\n",
    "# The ith axis is called the ith principal component (PC) of the data.\n",
    "# N.B. For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two\n",
    "# opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you\n",
    "# perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original\n",
    "# vectors. However, they will generally still lie on the same axes. In some cases, a pair of unit vectors may even rotate\n",
    "# or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "# The PCs are found via Singular Value Decomposition (SVD): this decomposes the training set matrix X into a matrix\n",
    "# multiplication of three matrices U Σ V⊺, where V contains the unit vectors that define all the principal components that\n",
    "# we are looking for (contained as the columns of V).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = 5*np.random.rand(10, 3)\n",
    "\n",
    "# PCA assumes the dataset is centered about the origin - sklearn PCA classes do this themselves but when using\n",
    "# personal implementations or other libraries it may be required to center the data first.\n",
    "\n",
    "# mean taken along the rows (collapse rows), so mean for each coordinate/feature.\n",
    "X_centered = X - X.mean(axis=0)\n",
    "# have to transpose Vt to get V (without any transpose), then each PC is a column of V\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "print(Vt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.699436Z",
     "start_time": "2020-08-03T22:46:38.695537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.87270059 4.75357153 3.65996971]\n",
      " [2.99329242 0.7800932  0.7799726 ]\n",
      " [0.29041806 4.33088073 3.00557506]]\n",
      "[[-2.42118524  0.95037868]\n",
      " [ 1.80006367 -1.21559131]\n",
      " [-2.81274807  0.67877657]]\n"
     ]
    }
   ],
   "source": [
    "# Once the PCs have been found, the dataset can be reduced to d dimensions by projecting it onto the hyperplane defined\n",
    "# by the first d PCs - this hyperplane preserves as much variance as possible (so the projection is as 'truthful' as \n",
    "# possible). Defining W_d as the matrix contatining the first d columns of V, we get the reduced X: X_dproj = X.W_d \n",
    "\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)\n",
    "print(X[:3], X2D[:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.707209Z",
     "start_time": "2020-08-03T22:46:38.700292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51194429, -0.17295472],\n",
       "       [ 0.85454939, -0.20232689],\n",
       "       [ 0.08751218,  0.96392453]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-Learn's PCA class uses SVD decomposition to implement PCA, as above. Once fitted, the components_ attribute holds\n",
    "# the transpose of W_d\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.712180Z",
     "start_time": "2020-08-03T22:46:38.708046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50114049, 0.35591274])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The explained variance ratio of each PC indicates the proportion of the dataset's varaince that lies along each PC.\n",
    "# This output, via the explained_variance_ratio_ attribute, tells us that 50.1% of the datasets variance lies along\n",
    "# the first PC and 35.6% along the second, leaving 14.3% for the third, suggesting the third PC carriers a reasonable\n",
    "# amount of information.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:38.721150Z",
     "start_time": "2020-08-03T22:46:38.713271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rather than arbitrarily choosing, it is often simpler to pick the number of dimensions that add up to a sufficiently \n",
    "# large proportion of the variance (e.g. 95%) - unless you are reducing the dimensionality for visualization (in which case\n",
    "# you want a dimensionality of 2 or 3).\n",
    "\n",
    "D = 10\n",
    "X_train = 10*np.random.rand(100, D)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "pca = PCA(n_components=d)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# An even simpler option is to set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you \n",
    "# wish to preserve.\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:51.302930Z",
     "start_time": "2020-08-03T22:46:38.722876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum) - there\n",
    "# will usually be an elbow in the curve where the explained variance stops growing fast.\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:55.037199Z",
     "start_time": "2020-08-03T22:46:51.303852Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum > 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:55.409398Z",
     "start_time": "2020-08-03T22:46:55.038017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# We can see that selecting n_components=100 will mean we preserve a reasonable amount of the dataset's variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:59.152922Z",
     "start_time": "2020-08-03T22:46:55.410291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that applying PCA to the MNIST dataset has compressed the data - preserving 95% of the variance means each instance\n",
    "# will have 150 features rather than the original 784, so the data takes up less than 20% of the original size. This\n",
    "# significant compression can lead to a good speed up in a classification algorithm (such as an SVM classifier).\n",
    "\n",
    "# We can also decompress the reduced dataset by applying the inverse transformation of the PCA projection. This does not \n",
    "# give back the original data, since the projection lost some information (within the 5% variance which was dropped), but\n",
    "# it will likely be close to the original data. X_recovered = X_dproj.W_d^T. \n",
    "# The mean squared distance between the original and reconstructed data is called the reconstruction error. \n",
    "\n",
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "# In this case there is some image quality loss, but the digits are mostly still intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:47:47.335914Z",
     "start_time": "2020-08-03T22:46:59.153733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 s ± 21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "3 s ± 22.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Setting the hyperparameter svd_solver='randomized' means sklearn uses a stochastic algorithm called Randomized PCA which\n",
    "# quickly finds an approximation of the first d PCs. It has computational complexity O(m x d^2) + O(d^3) instead of \n",
    "# O(m x n^2) + O(n^3) for the full SVD approach, so it is dramatically faster than full SVD when d << n.\n",
    "# The default for svd_solver is 'auto', sklearn automatically uses Randomized PCA if m or n is greater than 500 and \n",
    "# d is less than 80% of m or n. Otherwise it uses the full SVD approach. Setting svd_solver='full' forces sklearn to\n",
    "# use full SVD.\n",
    "\n",
    "pca = PCA(n_components=154, svd_solver='full')\n",
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "%timeit X_reduced = pca.fit_transform(X_train)\n",
    "%timeit X_reduced_rnd = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:47:55.322173Z",
     "start_time": "2020-08-03T22:47:47.643003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Incremental PCA algorithms allow you to split the training set into mini-batches and feed an IPCA algorithm one \n",
    "# mini-batch at a time. This is useful for large training sets and for applycing PCA online (as new instances arrive) - \n",
    "# the previous PCA implmentations require the whole training set to fit in memory to run.\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T22:46:30.985866Z",
     "start_time": "2020-08-03T22:46:30.982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively there is the Numpy memmap class, which allows you to manipulate a large array stored in a binary file on \n",
    "# disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. Since the \n",
    "# IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains under control. This\n",
    "# makes it possible to call the usual fit() method, as you can see in the following code:\n",
    "\"\"\"\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T23:21:07.181931Z",
     "start_time": "2020-08-03T23:21:07.150839Z"
    }
   },
   "outputs": [],
   "source": [
    "# In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly maps instances into a very \n",
    "# high-dimensional space (called the feature space), enabling nonlinear classification and regression with SVMs. \n",
    "# Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision\n",
    "# boundary in the original space. It turns out that the same trick can be applied to PCA, making it possible to perform\n",
    "# complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA).\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T23:24:06.877027Z",
     "start_time": "2020-08-03T23:24:05.624318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03555556, 0.04111111, 0.04666667, 0.05222222,\n",
       "       0.05777778, 0.06333333, 0.06888889, 0.07444444, 0.08      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel\n",
    "# and hyperparameter values. That said, dimensionality reduction is often a preparation step for a supervised learning task\n",
    "# (e.g., classification), so you can use grid search to select the kernel and hyperparameters that lead to the best \n",
    "# performance on that task. The following code creates a twostep pipeline, first reducing dimensionality to two dimensions\n",
    "# using kPCA, then applying Logistic Regression for classification. Then it uses GridSearchCV to find the best kernel \n",
    "# and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('kpca', KernelPCA(n_components=2)),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'kpca__gamma': np.linspace(0.03, 0.08, 10),\n",
    "    'kpca__kernel': ['rbf', 'sigmoid']\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T23:24:08.338664Z",
     "start_time": "2020-08-03T23:24:08.336813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.04666666666666666, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T23:26:19.849163Z",
     "start_time": "2020-08-03T23:26:19.761214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.852076810141359e-27"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters that yield the lowest\n",
    "# reconstruction error. Note that reconstruction is not as easy as with linear PCA. Thanks to the kernel trick, the \n",
    "# transformation from the original space to the reduced space involves an intermediate step: the feature map φ first \n",
    "# maps the training set to an infinite dimensional feature space, then we project the transformed training set down to\n",
    "# the 2D reduced space using linear PCA.\n",
    "# See Figure 8-11\n",
    "# Notice that if we could invert the final linear PCA step for a given instance in the reduced space, the reconstructed \n",
    "# point lies in the infinite-dimensional feature space, not in the original space, so we cannot compute the reconstructed \n",
    "# point, and therefore we cannot compute the true reconstruction error. \n",
    "# Fortunately, it is possible to find a point in the original space that would map close to the reconstructed point. \n",
    "# This point is called the reconstruction pre-image. Once you have this pre-image, you can measure its squared distance to\n",
    "# the original instance. You can then select the kernel and hyperparameters that minimize this reconstruction pre-image \n",
    "# error.\n",
    "# One way to perform this reconstruction is by training a supervised regression model, with the projected instances acting\n",
    "# as the training set and the original instances as targets. Scikit-Learn does this automatically if the hyperparameter \n",
    "# fit_inverse_transform=True. By default, fit_inverse_transform=False and KernelPCA has no inverse_transform() method. This method only gets created \n",
    "# when you set fit_inverse_transform=True.\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0466,\n",
    "                    fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "# Then, compute the reconstruction pre-image error, then use grid search with CV to find the kernel and hyperparameters \n",
    "# that minimize this error.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T23:32:52.021233Z",
     "start_time": "2020-08-03T23:32:51.955668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other than PCA, Locally Linear Embedding (LLE) is a powerful nonlinear dimensionality reduction (NLDR) technique. It is\n",
    "# a Manifold Learning technique that does not rely on projections - LLE works by first measuring how each training\n",
    "# instance linearly relates to its closest neighbours, and then the algorithm looks for a low-dimensional representation \n",
    "# of the training set where these local relationships are best preserved. This makes it particularly good at unrolling \n",
    "# twisted manifolds, especially when there is little noise.\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n",
    "\n",
    "# The resulting reduced dataset (from the initial Swiss roll) is completely unrolled, and the distances between instances\n",
    "# are locally well preserved. However, distances are not preserved on a larger scale: the left part of the unrolled Swiss\n",
    "# roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty good job at modeling the manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See book for complete LLE algorithm explaination.\n",
    "\n",
    "# Scikit-Learn’s LLE implementation has the following computational complexity: O(mlog(m) nlog(k)) for finding the\n",
    "# k nearest neighbors, O(mnk^3) for optimizing the weights, and O(dm^2) for constructing the low-dimensional \n",
    "# representations. Unfortunately, the m2 in the last term makes this algorithm scale poorly to very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Dimensionality Reduction Techniques\n",
    "\n",
    "# Random Projections\n",
    "# As its name suggests, projects the data to a lower-dimensional space using a random linear projection. It turns out that\n",
    "# such a random projection is actually very likely to preserve distances well, as was demonstrated mathematically by\n",
    "# William B. Johnson and Joram Lindenstrauss in a famous lemma. The quality of the dimensionality reduction depends on the\n",
    "# number of instances and the target dimensionality, but surprisingly not on the initial dimensionality.\n",
    "# Check out the documentation for the sklearn.random_projection package for more details.\n",
    "\n",
    "# Multidimensional Scaling (MDS)\n",
    "# Reduces dimensionality while trying to preserve the distances between the instances.\n",
    "\n",
    "# Isomap\n",
    "# Creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to\n",
    "# preserve the geodesic distances between the instances (the geodesic distance between two nodes in a graph is the number\n",
    "# of nodes on the shortest path between these nodes).\n",
    "\n",
    "# t-Distributed Stochastic Neighbor Embedding(t-SNE)\n",
    "# Reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used\n",
    "# for visualization, in particular to visualize clusters of instances in high-dimensional space(e.g., to visualize the\n",
    "# MNIST images in 2D).\n",
    "\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "# Is a classification algorithm, but during training it learns the most discriminative axes between the classes, and these\n",
    "# axes can then be used to define a hyperplane onto which to project the data. The benefit of this approach is that the\n",
    "# projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running\n",
    "# another classification algorithm such as an SVM classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
